\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{GW150914,GW170817,GWTC1,GWTC2,GWTC3,GWTC3_pop_analysis,GWTC2_GR,siren}
\citation{Thrane_2019}
\citation{LIGO_guide_signalextraction}
\citation{pycbc}
\citation{lal}
\citation{bilby_paper}
\citation{dynesty}
\citation{aLIGO,aVirgo,aLVK_prospects}
\citation{ET_science_case}
\citation{HuAccelerationReview}
\citation{wong2023fastgravitationalwaveparameter}
\citation{flowMC}
\citation{ripple}
\citation{bilby_paper}
\citation{jax2018github,cabezas2024blackjax}
\citation{yallup2025nested}
\citation{yallup2025nested}
\newlabel{firstpage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{skilling,Thrane_2019,lal,bilby_paper}
\citation{LIGO_guide_signalextraction}
\citation{Bayes1763}
\citation{LIGO_guide_signalextraction}
\citation{LIGO_guide_signalextraction}
\citation{skilling,dynamic_ns}
\citation{NSNature}
\citation{NS_methods_buchner}
\citation{GPU_computing}
\citation{CUDAGuide}
\citation{Handley:2015vkr,Smith:2019ucc}
\citation{yallup2025nested}
\citation{cabezas2024blackjax}
\citation{parallel_ns}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\newlabel{sec:background}{{2}{2}{}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Bayesian inference in GW astronomy}{2}{subsection.2.1}\protected@file@percent }
\newlabel{sec:background_bayes}{{2.1}{2}{}{subsection.2.1}{}}
\newlabel{eq:bayes_theorem}{{1}{2}{}{equation.1}{}}
\newlabel{eq:evidence}{{2}{2}{}{equation.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}GPU-accelerated nested sampling}{2}{subsection.2.2}\protected@file@percent }
\newlabel{sec:background_ns_and_gpus}{{2.2}{2}{}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}The nested sampling algorithm}{2}{subsubsection.2.2.1}\protected@file@percent }
\newlabel{sec:background_ns}{{2.2.1}{2}{}{subsubsection.2.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}GPU architectures for scientific computing}{2}{subsubsection.2.2.2}\protected@file@percent }
\newlabel{sec:background_gpus}{{2.2.2}{2}{}{subsubsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}A vectorized formulation of nested sampling}{2}{subsubsection.2.2.3}\protected@file@percent }
\newlabel{sec:background_vectorized_ns}{{2.2.3}{2}{}{subsubsection.2.2.3}{}}
\citation{pmlr-v151-hoffman22a,pmlr-v130-hoffman21a}
\citation{yallup2025nested}
\citation{Neal2003_slice}
\citation{bilby_paper,dynesty}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Recovered posteriors for the 4s signal. The posteriors are in excellent agreement with each other, demonstrating that the \texttt  {blackjax-ns} implementation with our custom kernel is functionally equivalent to the \texttt  {bilby} + \texttt  {dynesty} implementation.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:4s_posteriors}{{1}{3}{Recovered posteriors for the 4s signal. The posteriors are in excellent agreement with each other, demonstrating that the \texttt {blackjax-ns} implementation with our custom kernel is functionally equivalent to the \texttt {bilby} + \texttt {dynesty} implementation}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of the number of live points in the sequential CPU and batched GPU implementations. Although the nominal number of live points used in \texttt  {blackjax-ns} is higher than in \texttt  {bilby}, the saw-tooth pattern means that the effective number of live points is the same. Here, we use a batch size of $k = 0.5 \times n_{\text  {live}}$.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:nlive_comparison}{{2}{3}{Comparison of the number of live points in the sequential CPU and batched GPU implementations. Although the nominal number of live points used in \texttt {blackjax-ns} is higher than in \texttt {bilby}, the saw-tooth pattern means that the effective number of live points is the same. Here, we use a batch size of $k = 0.5 \times n_{\text {live}}$}{figure.2}{}}
\citation{DE,DE2}
\citation{dau2021wastefreesequentialmontecarlo}
\citation{yallup2025nested}
\citation{Handley:2015vkr}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{4}{section.3}\protected@file@percent }
\newlabel{sec:methods}{{3}{4}{}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The inner sampling kernel}{4}{subsection.3.1}\protected@file@percent }
\newlabel{sec:methods_kernel}{{3.1}{4}{}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Sampler configuration and settings}{4}{subsection.3.2}\protected@file@percent }
\newlabel{sec:sampler_config}{{3.2}{4}{}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of the sample weights for each dead point for the sequential CPU and batched GPU implementations. The weights are calculated using the prior volumes enclosed between successive dead points and the likelihoods. The shapes are similar, and both implementations enter the bulk of the posterior distribution at similar iterations, indiciating that setting the number of live points in \texttt  {blackjax-ns} to 1.4 times the number of live points in \texttt  {bilby} does indeed result in a like-for-like comparison. The same saw-tooth pattern can be seen in the weights for the \texttt  {blackjax-ns} implementation.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:weights_comparison}{{3}{4}{Comparison of the sample weights for each dead point for the sequential CPU and batched GPU implementations. The weights are calculated using the prior volumes enclosed between successive dead points and the likelihoods. The shapes are similar, and both implementations enter the bulk of the posterior distribution at similar iterations, indiciating that setting the number of live points in \texttt {blackjax-ns} to 1.4 times the number of live points in \texttt {bilby} does indeed result in a like-for-like comparison. The same saw-tooth pattern can be seen in the weights for the \texttt {blackjax-ns} implementation}{figure.3}{}}
\citation{skilling,dynamic_ns,aeons}
\citation{ripple,wong2023fastgravitationalwaveparameter,Wouters_BNS}
\citation{TL_relativebinning,relativebinning2,relativebinning3,relativebinning4}
\citation{wong2023fastgravitationalwaveparameter}
\citation{Khan:2015jqa}
\citation{bilby_validation}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Setting $n_{\text  {GPU}}$}{5}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{sec:setting_n_gpu}{{3.2.1}{5}{}{subsubsection.3.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Likelihood}{5}{subsection.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Prior distributions for the parameters of the binary black hole system. The specific ranges for the masses and spins are dependent on the signal and are specified in Section~\ref {sec:results}.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:priors}{{1}{5}{Prior distributions for the parameters of the binary black hole system. The specific ranges for the masses and spins are dependent on the signal and are specified in Section~\ref {sec:results}}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Priors}{5}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and discussion}{5}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{5}{}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Simulated signals}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}4-second simulated signal}{6}{subsubsection.4.1.1}\protected@file@percent }
\newlabel{sec:4s_simulated_signal}{{4.1.1}{6}{}{subsubsection.4.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Injection Study}{6}{subsection.4.2}\protected@file@percent }
\newlabel{sec:injection_study}{{4.2}{6}{}{subsection.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Injection parameters for the 4s signal.}}{6}{table.2}\protected@file@percent }
\newlabel{tab:injection_params}{{2}{6}{Injection parameters for the 4s signal}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of the log evidence for the 4s signal. The results are in excellent agreement, demonstrating the robustness of the \texttt  {blackjax-ns} implementation in recovering the same posteriors and evidence as the \texttt  {bilby} implementation. This unifies parameter estimation and evidence evaluation into a single GPU-accelerated framework.}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:4s_logZ_comparison}{{4}{6}{Comparison of the log evidence for the 4s signal. The results are in excellent agreement, demonstrating the robustness of the \texttt {blackjax-ns} implementation in recovering the same posteriors and evidence as the \texttt {bilby} implementation. This unifies parameter estimation and evidence evaluation into a single GPU-accelerated framework}{figure.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of the sampling times and cost savings for the 4s signal.}}{6}{table.3}\protected@file@percent }
\newlabel{tab:4s_time_comparison}{{3}{6}{Comparison of the sampling times and cost savings for the 4s signal}{table.3}{}}
\citation{Prathaban_2025_zenodo}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution of the network signal-to-noise ratios (SNR) for the injected signals.}}{7}{figure.5}\protected@file@percent }
\newlabel{fig:snr_dist}{{5}{7}{Distribution of the network signal-to-noise ratios (SNR) for the injected signals}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of the accepted number of steps per iteration for the sequential CPU and batched GPU analyses of the first signal from the injection study. We adapt our `delay' parameter from the tuning formula such that we obtain similar accepted steps for the two implementations. The \texttt  {blackjax-ns} implementation can only perform tuning every \mbox  {\texttt  {num\_delete}} iterations, so we tune the chain length more aggressively.}}{7}{figure.6}\protected@file@percent }
\newlabel{fig:chain_length_comparison}{{6}{7}{Comparison of the accepted number of steps per iteration for the sequential CPU and batched GPU analyses of the first signal from the injection study. We adapt our `delay' parameter from the tuning formula such that we obtain similar accepted steps for the two implementations. The \texttt {blackjax-ns} implementation can only perform tuning every \mbox {\texttt {num\_delete}} iterations, so we tune the chain length more aggressively}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Percentile-percentile (PP) coverage plot for the 100-injection study, obtained with the \texttt  {blackjax-ns} sampler. The cumulative fraction of events where the true injected parameter falls below a given credible level is plotted against that credible level. The proximity of all parameter curves to the diagonal indicates that the posterior credible intervals are statistically well-calibrated. A corresponding plot for the \texttt  {bilby+dynesty} analysis, which should be identical, is provided in the Appendix for reference.}}{7}{figure.7}\protected@file@percent }
\newlabel{fig:pp_coverage}{{7}{7}{Percentile-percentile (PP) coverage plot for the 100-injection study, obtained with the \texttt {blackjax-ns} sampler. The cumulative fraction of events where the true injected parameter falls below a given credible level is plotted against that credible level. The proximity of all parameter curves to the diagonal indicates that the posterior credible intervals are statistically well-calibrated. A corresponding plot for the \texttt {bilby+dynesty} analysis, which should be identical, is provided in the Appendix for reference}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The wall-time speedups for all 100 events in the injection study.}}{7}{figure.8}\protected@file@percent }
\newlabel{fig:speedup_comparison}{{8}{7}{The wall-time speedups for all 100 events in the injection study}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The cost reductions for all 100 events in the injection study.}}{7}{figure.9}\protected@file@percent }
\newlabel{fig:cost_reduction}{{9}{7}{The cost reductions for all 100 events in the injection study}{figure.9}{}}
\citation{yallup2025nested}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Disentangling Sources of GPU Acceleration}{8}{subsection.4.3}\protected@file@percent }
\newlabel{sec:disentangling_acceleration}{{4.3}{8}{}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}8-second simulated signal}{8}{subsection.4.4}\protected@file@percent }
\newlabel{sec:8s_simulated_signal}{{4.4}{8}{}{subsection.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Injection parameters for the 8s signal.}}{8}{table.4}\protected@file@percent }
\newlabel{tab:8s_injection_params}{{4}{8}{Injection parameters for the 8s signal}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{8}{section.5}\protected@file@percent }
\newlabel{sec:conclusions}{{5}{8}{}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Recovered posterior distributions for the 8s simulated signal, comparing our GPU-based \texttt  {blackjax-ns} sampler with the CPU-based \texttt  {bilby} sampler. The injected values are marked by black lines. The strong statistical agreement confirms the validity of our implementation for longer-duration signals. The GPU implementation provided a wall-time speedup of 46$\times $ and a cost reduction of 2.9$\times $.}}{9}{figure.10}\protected@file@percent }
\newlabel{fig:8s_posteriors}{{10}{9}{Recovered posterior distributions for the 8s simulated signal, comparing our GPU-based \texttt {blackjax-ns} sampler with the CPU-based \texttt {bilby} sampler. The injected values are marked by black lines. The strong statistical agreement confirms the validity of our implementation for longer-duration signals. The GPU implementation provided a wall-time speedup of 46$\times $ and a cost reduction of 2.9$\times $}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comparison of the recovered log-evidence ($Z$) for the 8s signal. The results from both the \texttt  {bilby} and \texttt  {blackjax-ns} frameworks are consistent within their estimated numerical uncertainties.}}{9}{figure.11}\protected@file@percent }
\newlabel{fig:8s_logZ}{{11}{9}{Comparison of the recovered log-evidence ($Z$) for the 8s signal. The results from both the \texttt {bilby} and \texttt {blackjax-ns} frameworks are consistent within their estimated numerical uncertainties}{figure.11}{}}
\citation{yallup2025nested}
\citation{Williams2021Nessai}
\citation{Prathaban}
\citation{Prathaban_2025_zenodo}
\citation{blackjax_ns_github}
\citation{Prathaban_2025_github}
\bibstyle{mnras}
\bibdata{references}
\bibcite{GW150914}{{1}{2016}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{GW170817}{{2}{2017a}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{siren}{{3}{2017b}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{GWTC1}{{4}{2019}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{aLVK_prospects}{{5}{2020a}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{LIGO_guide_signalextraction}{{6}{2020b}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{GWTC2_GR}{{7}{2021}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{GWTC3_pop_analysis}{{8}{2023a}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{GWTC3}{{9}{2023b}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{GWTC2}{{10}{2024}{{Abbott et~al.}}{{Abbott et~al.}}}
\bibcite{aVirgo}{{11}{2014}{{Acernese et~al.}}{{Acernese et~al.,}}}
\bibcite{bilby_paper}{{12}{2019}{{Ashton et~al.}}{{Ashton et~al.}}}
\bibcite{NSNature}{{13}{2022}{{Ashton et~al.}}{{Ashton, Bernstein, Buchner \& et al.}}}
\bibcite{Bayes1763}{{14}{1763}{{{Bayes}}}{{{Bayes}}}}
\bibcite{pycbc}{{15}{2019}{{{Biwer} et~al.}}{{{Biwer}, {Capano}, {De}, {Cabero}, {Brown}, {Nitz} \& {Raymond}}}}
\bibcite{jax2018github}{{16}{2018}{{Bradbury et~al.}}{{Bradbury et~al.,}}}
\bibcite{ET_science_case}{{17}{2023}{{Branchesi et~al.}}{{Branchesi et~al.}}}
\bibcite{NS_methods_buchner}{{18}{2023}{{Buchner}}{{Buchner}}}
\bibcite{cabezas2024blackjax}{{19}{2024}{{Cabezas et~al.}}{{Cabezas, Corenflos, Lao \& Louf}}}
\bibcite{relativebinning4}{{20}{2013}{{Cornish}}{{Cornish}}}
\bibcite{dau2021wastefreesequentialmontecarlo}{{21}{2021}{{Dau \& Chopin}}{{Dau \& Chopin}}}
\bibcite{ripple}{{22}{2024}{{Edwards et~al.}}{{Edwards, Wong, Lam, Coogan, Foreman-Mackey, Isi \& Zimmerman}}}
\bibcite{Handley:2015vkr}{{23}{2015}{{Handley et~al.}}{{Handley, Hobson \& Lasenby}}}
\bibcite{parallel_ns}{{24}{2014}{{Henderson \& Goggans}}{{Henderson \& Goggans}}}
\bibcite{Higson_Errors}{{25}{2018}{{Higson et~al.}}{{Higson, Handley, Hobson \& Lasenby}}}
\bibcite{dynamic_ns}{{26}{2019}{{Higson et~al.}}{{Higson, Handley, Hobson \& Lasenby}}}
\bibcite{pmlr-v151-hoffman22a}{{27}{2022}{{Hoffman \& Sountsov}}{{Hoffman \& Sountsov}}}
\bibcite{pmlr-v130-hoffman21a}{{28}{2021}{{Hoffman et~al.}}{{Hoffman, Radul \& Sountsov}}}
\bibcite{HuAccelerationReview}{{29}{2024}{{Hu \& Veitch}}{{Hu \& Veitch}}}
\bibcite{aeons}{{30}{2024}{{Hu et~al.}}{{Hu, Baryshnikov \& Handley}}}
\bibcite{Khan:2015jqa}{{31}{2016}{{Khan et~al.}}{{Khan, Husa, Hannam, Ohme, P\"urrer, Jim\'enez~Forteza \& Boh\'e}}}
\bibcite{TL_relativebinning}{{32}{2023}{{Krishna et~al.}}{{Krishna, Vijaykumar, Ganguly, Talbot, Biscoveanu, George, Williams \& Zimmerman}}}
\bibcite{relativebinning3}{{33}{2021}{{Leslie et~al.}}{{Leslie, Dai \& Pratten}}}
\bibcite{CUDAGuide}{{34}{2025}{{{NVIDIA Corporation}}}{{{NVIDIA Corporation}}}}
\bibcite{Neal2003_slice}{{35}{2003}{{Neal}}{{Neal}}}
\bibcite{GPU_computing}{{36}{2008}{{Owens et~al.}}{{Owens, Houston, Luebke, Green, Stone \& Phillips}}}
\bibcite{Prathaban_PE_errors}{{37}{2024}{{Prathaban \& Handley}}{{Prathaban \& Handley}}}
\bibcite{Prathaban_2025_github}{{38}{2025b}{{Prathaban et~al.}}{{Prathaban et~al.}}}
\bibcite{Prathaban_2025_zenodo}{{39}{2025a}{{Prathaban et~al.}}{{Prathaban, Yallup, Alvey, Yang, Templeton \& Handley}}}
\bibcite{Prathaban}{{40}{2025c}{{Prathaban et~al.}}{{Prathaban, Bevins \& Handley}}}
\bibcite{bilby_validation}{{41}{2020}{{Romero-Shaw et~al.}}{{Romero-Shaw et~al.,}}}
\bibcite{skilling}{{42}{2006}{{Skilling}}{{Skilling}}}
\bibcite{Smith:2019ucc}{{43}{2020}{{Smith et~al.}}{{Smith, Ashton, Vajpeyi \& Talbot}}}
\bibcite{dynesty}{{44}{2020}{{Speagle}}{{Speagle}}}
\bibcite{DE}{{45}{1997}{{Storn \& Price}}{{Storn \& Price}}}
\bibcite{aLIGO}{{46}{2015}{{{The LIGO Scientific Collaboration} et~al.}}{{{The LIGO Scientific Collaboration} et~al.}}}
\bibcite{Thrane_2019}{{47}{2019}{{Thrane \& Talbot}}{{Thrane \& Talbot}}}
\bibcite{lal}{{48}{2015}{{Veitch et~al.}}{{Veitch et~al.,}}}
\bibcite{Williams2021Nessai}{{49}{2021}{{{Williams} et~al.}}{{{Williams}, {Veitch} \& {Messenger}}}}
\bibcite{wong2023fastgravitationalwaveparameter}{{50}{2023a}{{Wong et~al.}}{{Wong, Isi \& Edwards}}}
\bibcite{flowMC}{{51}{2023b}{{Wong et~al.}}{{Wong, Gabri\'e \& Foreman-Mackey}}}
\bibcite{Wouters_BNS}{{52}{2024}{{Wouters et~al.}}{{Wouters, Pang, Dietrich \& Van Den~Broeck}}}
\bibcite{blackjax_ns_github}{{53}{2025b}{{Yallup et~al.}}{{Yallup, Handley, Cabezas, et~al.}}}
\bibcite{yallup2025nested}{{54}{2025a}{{Yallup et~al.}}{{Yallup, Kroupa \& Handley}}}
\bibcite{relativebinning2}{{55}{2018}{{Zackay et~al.}}{{Zackay, Dai \& Venumadhav}}}
\bibcite{DE2}{{56}{2006}{{{ter Braak}}}{{{ter Braak}}}}
\citation{skilling,Higson_Errors}
\citation{Prathaban_PE_errors}
\@writefile{toc}{\contentsline {section}{\numberline {A}PP coverage plot for \texttt  {bilby} and \texttt  {dynesty}}{11}{section.A1}\protected@file@percent }
\newlabel{app:bilby_pp_plot}{{A}{11}{}{section.A1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Full run statistics for injection study}{11}{section.A2}\protected@file@percent }
\newlabel{app:injection_study_stats}{{B}{11}{}{section.A2}{}}
\newlabel{lastpage}{{B}{11}{}{figure.A2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A1}{\ignorespaces PP coverage plot for the 100-injection study, obtained with the CPU-based \texttt  {bilby+dynesty} sampler. This plot is provided for direct comparison with the results from our \texttt  {blackjax-ns} implementation shown in Figure~\ref {fig:pp_coverage}. The results confirm that the CPU-based and GPU-based implementations are functionally similar.}}{12}{figure.A1.1}\protected@file@percent }
\newlabel{fig:pp_coverage_bilby}{{A1}{12}{PP coverage plot for the 100-injection study, obtained with the CPU-based \texttt {bilby+dynesty} sampler. This plot is provided for direct comparison with the results from our \texttt {blackjax-ns} implementation shown in Figure~\ref {fig:pp_coverage}. The results confirm that the CPU-based and GPU-based implementations are functionally similar}{figure.A1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B1}{\ignorespaces Comparison of internal run statistics for the 100-injection study. From left to right: the mean number of likelihood evaluations per iteration, the mean number of accepted steps, and the mean acceptance rate, for both the \texttt  {bilby} and \texttt  {blackjax-ns} runs. The results show that despite the differences in tuning strategy, motivated by architectural differences between the CPU and GPU, the two samplers exhibit similar properties.}}{13}{figure.A2.1}\protected@file@percent }
\newlabel{fig:injection_study_stats}{{B1}{13}{Comparison of internal run statistics for the 100-injection study. From left to right: the mean number of likelihood evaluations per iteration, the mean number of accepted steps, and the mean acceptance rate, for both the \texttt {bilby} and \texttt {blackjax-ns} runs. The results show that despite the differences in tuning strategy, motivated by architectural differences between the CPU and GPU, the two samplers exhibit similar properties}{figure.A2.1}{}}
\gdef \@abspage@last{13}
